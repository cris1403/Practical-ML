{"name":"Practical-ml","tagline":"Practical Machine Learning on Coursera","body":"Weight Lifting Exercises\r\n===========================\r\nHuman Activity Recognition\r\n---------------------------\r\n\r\nHuman Activity Recognition has become a challenging application which involves\r\nthe use of different technologies to automatically collect and classify human activities for\r\ndifferent application domains, ranging from medical applications, home monitoring and assisted living.\r\n\r\nIn this task, six young health participants were asked to perform one set of 10 repetitions of the Unilateral Dumbbell Biceps Curl in five different fashions: exactly according to the specification (Class A), throwing the elbows to the front (Class B), lifting the dumbbell only halfway (Class C), lowering the dumbbell only halfway (Class D) and throwing the hips to the front (Class E).\r\nClass A corresponds to the specified execution of the exercise, while the other 4 classes correspond to common mistakes. \r\nOur aim is to predict how well an activity was performed by a participant.\r\n(Read more: http://groupware.les.inf.purio.br/har#weight_lifting_exercises#ixzz3Deo2ygkn)\r\n\r\nLet's download the train dataset and the test dataset from the web.\r\n```{r message=FALSE, warning=FALSE}\r\nrequire(caret)\r\nrequire(gbm)\r\nrequire(pROC)\r\nrequire(randomForest)\r\n\r\ntrain = read.csv(\"C:/Users/Cris/Desktop/MOOC/Practical Machine Learning/Project/data/pml-training.csv\", sep=\",\")\r\ntest = read.csv(\"C:/Users/Cris/Desktop/MOOC/Practical Machine Learning/Project/data/pml-testing.csv\", sep=\",\")\r\n\r\ndim(train)\r\ndim(test)\r\n```\r\nFeature selection\r\n------------------\r\nOur train dataset has 19,622 observations and 160 variables. Having examined summary tables, we decide to delete some useless variables with spare distributions (reporting statistics as min, max, kurtosis, skewness). We also use unsupervised filters to remove predictors with high inter-predictor correlations which wouldn't give us additional information about the classe/target variable.\r\n\r\n```{r warning=FALSE}\r\ntraining = train[,c(8:11,37:49,60:68,84:86,102,113:124,140,151:160)]\r\ntesting = test[,c(8:11,37:49,60:68,84:86,102,113:124,140,151:159)] #160 is not classe\r\n\r\ncorrelations  = cor(training[,-53])\r\nhighlyCorrelated = findCorrelation(correlations, cutoff = .85)\r\n\r\ntrain.set = training[,-highlyCorrelated]\r\ntest.set  = testing[,-highlyCorrelated]\r\n```\r\nModel\r\n----------------\r\nThe learning model is random forests, an algorithm that build lots of bushy trees, and then average them to reduce the variance. We also have tried Gradient Boosting Machine, which gave amazing performance on this dataset as well. Boosting builds lots of smaller trees. Unlike random forests, each new tree in boosting tries to patch up the deficiencies of the current ensemble.\r\n\r\nModel Training\r\n-----------------\r\nParameter Tuning\r\n-----------------\r\nRandom forest has only one tuning parameter, 'mtry', which controls the number of features selected for each tree. In order to prevent overfitting, two separate 10-fold cross-validation are used as the resampling scheme to choose the best parameter.\r\nFor each iteration, the strategy is to hold out a cross-validation dataset. Then, for each \"mtry\" value, the algorithm fits the model on 9-fold dataset and predict the classe variable on the hold-out dataset. \r\n```{r warning=FALSE}\r\nfitControl = trainControl(method='repeatedcv', number=10,\r\n                          repeats=2, returnResamp='all')\r\nrfGrid   = expand.grid(.mtry=c(2,5,8))\r\nrf.tune = train(x=train.set[,1:44], \r\n                y=train.set[,45], \r\n                method='rf', \r\n                trControl=fitControl,\r\n                tuneGrid=rfGrid)\r\nrf.tune\r\n```\r\n\r\n```{r comment=NA, fig.width=7}\r\nplot(rf.tune)\r\n```\r\n\r\nIn the model output, there's a row for each mtry values. The \"Accuracy\" column is the average accuracy of the held-out samples. The optimal model is the one with the highest Accuracy.\r\nThe best model is the one with `mtry=8`. Since $p=44$ here, we could have tried all 44 possible values of `mtry`. Caret records the results and picks the best model which doesn't look very complex. Then we think it won't overfit future data. In other words, it'll generalize well.\r\n\r\nPrediction of new samples\r\n------------------------------\r\nWe apply the optimal model to new cases, to predict if the participant is doing the exercise well or not.\r\nAs we know that $error=1-accuracy$, we already know the cross-validation set error, which is an optimistic estimate of the out of sample dataset (new cases). In this task, the model gives an extremely high accuracy, then we know the model is going to make very good predictions.\r\n```{r warning=FALSE}\r\ntest.pred = predict(rf.tune, newdata=test.set)\r\ntest.pred\r\n```","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}